\chapter{Machine Learning}

\begin{defi}[Learning algorithm for a meta-domain]%Problem: SxG??
Formally, an algorithm $A$ is a \term{learning algorithm for a meta-domain} $M$ in a hypothesis space $H$ with respect to a set of problem distributions $T$, if for any domain $D\in M$, any choice of a problem distribution $P$ in $T$, and any target problem solver $f\in H$,
\begin{enumerate}
\item $A$ takes as input the specification of a domain $D\in M$, an error parameter $E$, and a confidence parameter $\sigma$,
\item $A$ may call \alg{SolvedProblem}, which returns examples $\brak{x,\ffun{x}}$ for $D$, where $x$ is chosen with probability $\Pfun{x}$ from $S\times G$. The number of oracle calls of $A$ and its running time must be polynomial in the maximum problem size and the length of its input.
\item For all $D\in M$ and distributions $P\in T$, with probability at least $\brak{1-\delta}A$ outputs a program $f'$ that approximates $f$ in the sense that
\begin{equation}
\displaystyle\sum_{x\in\Delta}{\Pfun{x}}\leq\epsilon
\end{equation}
where $\Delta=\accol{x | f'\mbox{ fails on }x\mbox{ while }f\mbox{ succeeds}}$.
\item There is a polynomial $R$ such that, for a maximum problem size $n,\frac{1}{\epsilon},|frac{1}{\delta}$, maximum length $I$ and maximum step length $r$ of any solution output by \alg{SolvedProblem}, and an upper bound $t$ on the running times of programs in $D$ on inputs of size $n$, if $A$ outputs $f'$, the run time of $f'$ is bounded by $\fun{R}{n,l,r,t,\frac{1}{\epsilon},\frac{1}{\delta}}$
\end{enumerate}
\cite{conf/ijcai/Tadepalli91}
\end{defi}

\begin{defi}[Satisfying a spare solution space bias]
A problem solver $f$ for a domain $D$ and a problem distribution $P$ \term[Satisfying a sparse solution space bias]{satisfies a sparse solution space bias} if there is a set of operator sequences $m_f$ such that, on any problem $x\in D$ such that $\Pfun{x}>0$, $\ffun{x}\in m_f$ and $\abs{m_f}$ is bounded by a polynomial $Q$ in the problem size $n$.
\cite{conf/ijcai/Tadepalli91}
\end{defi}

\begin{defi}[Satisfying a macro table bias]
A problem solver $f$ \term[Satisfying a macro table bias]{satisfies a macro table bias} for a domain $D$ in $M$ if there is a feature ordering $O=\brak{l,\ldots,n}$ such that,
\begin{enumerate}
 \item $D$ is serially decomposable for $O$, and
 \item $f$ constructs all its solutions using a macro table $M$ as follows: for each feature $i$ from $1$ to $n$, macros $M_{j,i}$ are successively applied, where $j$ is the value of feature $i$ in the state before applying the macro.
\end{enumerate}
\cite{conf/ijcai/Tadepalli91}
\end{defi}

\begin{defi}[Beliefs in Conjoint Analysis]
We allow weighted beliefs with a weight parameter coming from \ocinterval{0}{1} where $1$ means full truth degree (complete certainty, the perfect belief), while a value $\alpha\in\oointerval{0}{1}$ describe a regular belief that can be doubted.
\begin{enumerate}
 \item \term{Regular belief}s such as:
 \begin{equation}
(\fun{A_1}{a_1}\wedge\ldots\wedge\fun{A_t}{a_t}):\alpha
 \end{equation}
 \item \term{Indifference belief}s such as:
 \begin{equation}
 \left(L\leftrightarrow R\right):1
 \end{equation}
 Indifference beliefs are always have full truth because we claim that if the respondent would distinguish degrees of truth then she is able to express preference.
 \item \term{Negative belief}s such as:
 \begin{equation}
 \left(\neg F\right):1
 \end{equation}
\end{enumerate}
where $A_i$ are attribute predicates and $L$, $R$, $F$ are regular atom conjunctions. Again, it is obvious in conjoint to don't ask user to express thoughts on negative information. As such there are no real negative beliefs such as $F:0$. Moreover, the reader may notice that we adopt the intuitionistic logic approach i.e., there is no assumption on any kind of law of excluded middle, as we don't necessarily assume $F:0\Leftrightarrow\left(\neg F\right):1$.
\cite{conf/fedcsis/GiurcaSB12}
\end{defi}

\section{Empirical Law Discovery}

\begin{defi}[$X$-of-$N$ representation]
Let \accol{A_i|1\leq i\leq\mbox{MaxAtt}} be the set of attributes of a domain, and for each $A_i$, \accol{V_{ij}|1\leq j\leq\mbox{MaxAttVal}_i} be its value set where $\mbox{MaxAtt}$ is the number of attributes, and $\mbox{MaxAttVal}_i$, is the number of different values of $A_i$. An \term{$X$-of-$N$ representation} is a set, denoted as $\mbox{$X$-of-}\accol{AV_k|AV_k\mbox{ is an attribute-value pair denoted as ``}A_i=V_{i,j}\mbox{'', }1\leq k\leq N_+, N\leq N_+, 1\leq N\leq\mbox{MaxAtt}}$, where $N_+$ is the number of attribute-value pairs in the $X$-of-$N$ representation, called the size of the $X$-of-$N$ representation, and $N$ is the number of different attributes that appear in the $X$-of-$N$ representation. The value of an $X$-of-$N$ can be any number between $0$ and $N$. Given an instance, its value is $X$ \iffTx{} $X$ of the $AV_k$ are true. An attribute-value pair $\fun{AV_k}{A_i=V_{ij}}$ is true for an instance \iffTx{} attribute $A_i$, of the instance has value $V_{ij}$.
\cite{conf/ijcai/Zheng95}
\end{defi}