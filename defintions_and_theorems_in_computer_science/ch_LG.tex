\chapter{Machine Learning}

\begin{defi}[Learning algorithm for a meta-domain]%Problem: SxG??
Formally, an algorithm $A$ is a \term{learning algorithm for a meta-domain} $M$ in a hypothesis space $H$ with respect to a set of problem distributions $T$, if for any domain $D\in M$, any choice of a problem distribution $P$ in $T$, and any target problem solver $f\in H$,
\begin{enumerate}
\item $A$ takes as input the specification of a domain $D\in M$, an error parameter $E$, and a confidence parameter $\sigma$,
\item $A$ may call \alg{SolvedProblem}, which returns examples $\brak{x,\ffun{x}}$ for $D$, where $x$ is chosen with probability $\Pfun{x}$ from $S\times G$. The number of oracle calls of $A$ and its running time must be polynomial in the maximum problem size and the length of its input.
\item For all $D\in M$ and distributions $P\in T$, with probability at least $\brak{1-\delta}A$ outputs a program $f'$ that approximates $f$ in the sense that
\begin{equation}
\displaystyle\sum_{x\in\Delta}{\Pfun{x}}\leq\epsilon
\end{equation}
where $\Delta=\accol{x | f'\mbox{ fails on }x\mbox{ while }f\mbox{ succeeds}}$.
\item There is a polynomial $R$ such that, for a maximum problem size $n,\frac{1}{\epsilon},|frac{1}{\delta}$, maximum length $I$ and maximum step length $r$ of any solution output by \alg{SolvedProblem}, and an upper bound $t$ on the running times of programs in $D$ on inputs of size $n$, if $A$ outputs $f'$, the run time of $f'$ is bounded by $\fun{R}{n,l,r,t,\frac{1}{\epsilon},\frac{1}{\delta}}$
\end{enumerate}
\cite{conf/ijcai/Tadepalli91}
\end{defi}

\begin{defi}[Satisfying a spare solution space bias]
A problem solver $f$ for a domain $D$ and a problem distribution $P$ \term[Satisfying a sparse solution space bias]{satisfies a sparse solution space bias} if there is a set of operator sequences $m_f$ such that, on any problem $x\in D$ such that $\Pfun{x}>0$, $\ffun{x}\in m_f$ and $\abs{m_f}$ is bounded by a polynomial $Q$ in the problem size $n$.
\cite{conf/ijcai/Tadepalli91}
\end{defi}

\begin{defi}[Satisfying a macro table bias]
A problem solver $f$ \term[Satisfying a macro table bias]{satisfies a macro table bias} for a domain $D$ in $M$ if there is a feature ordering $O=\brak{l,\ldots,n}$ such that,
\begin{enumerate}
 \item $D$ is serially decomposable for $O$, and
 \item $f$ constructs all its solutions using a macro table $M$ as follows: for each feature $i$ from $1$ to $n$, macros $M_{j,i}$ are successively applied, where $j$ is the value of feature $i$ in the state before applying the macro.
\end{enumerate}
\cite{conf/ijcai/Tadepalli91}
\end{defi}

\begin{defi}[Beliefs in Conjoint Analysis]
We allow weighted beliefs with a weight parameter coming from \ocinterval{0}{1} where $1$ means full truth degree (complete certainty, the perfect belief), while a value $\alpha\in\oointerval{0}{1}$ describe a regular belief that can be doubted.
\begin{enumerate}
 \item \term{Regular belief}s such as:
 \begin{equation}
(\fun{A_1}{a_1}\wedge\ldots\wedge\fun{A_t}{a_t}):\alpha
 \end{equation}
 \item \term{Indifference belief}s such as:
 \begin{equation}
 \left(L\leftrightarrow R\right):1
 \end{equation}
 Indifference beliefs are always have full truth because we claim that if the respondent would distinguish degrees of truth then she is able to express preference.
 \item \term{Negative belief}s such as:
 \begin{equation}
 \left(\neg F\right):1
 \end{equation}
\end{enumerate}
where $A_i$ are attribute predicates and $L$, $R$, $F$ are regular atom conjunctions. Again, it is obvious in conjoint to don't ask user to express thoughts on negative information. As such there are no real negative beliefs such as $F:0$. Moreover, the reader may notice that we adopt the intuitionistic logic approach i.e., there is no assumption on any kind of law of excluded middle, as we don't necessarily assume $F:0\Leftrightarrow\left(\neg F\right):1$.
\cite{conf/fedcsis/GiurcaSB12}
\end{defi}

\begin{defi}[Single-controller-stochastic-game, stage game, agent, adversary, probabilistic transition function]
A \termabbrev{single-controller-stochastic-game}{SCSG} $M$ on states $S=\accol{1,2,\ldots,N}$ and actions $A=\accol{a_1,a_2,\ldots,a_k}$, consists of:
\begin{enumerate}
 \item \term[Stage game]{Stage games}: each state $s\in S$ is associated with a zero-sum game in strategic form, where the action set of each player is $A$. The first player is termed \term{agent} and the second player is termed \term{adversary}.
 \item \term{Probabilistic transition function}: $\fun{P_M}{s,t,a}$ is the probability of a transition from $s$ to $t$ given that the first player (termed agent), plays $a$.
\end{enumerate}
\cite{Brafman:1999:NPA:1624312.1624324}
\end{defi}

\begin{defi}[$\alpha$-approximation of a single-controller-stochastic-game]
Let $M$ and $\overline{M}$ be single-controller-stochastic-games over the same state space. We say that $\overline{M}$ is an \term[$\alpha$-approximation of a single-controller-stochastic-game]{$\alpha$-approximation of $M$} if for every state $s$ we have:
\begin{enumerate}
 \item If \fun{P_M}{s,t,a} and \fun{P_{\overline{M}}}{s,t,a} are the probabilities of transition to $t$ given that the action carried out by the agent is $a$, in $M$ and $\overline{M}$ respectively, then, $\fun{P_M}{s,t,a}-\alpha\leq \fun{P_{\overline{M}}}{s,t,a}\leq \fun{P_M}{s,t,a}+\alpha$
 \item The game associated with $s$ in $\overline{M}$ is the game associated with it in $M$ restricted to a non-empty subset of the columns.
\end{enumerate}
\cite{Brafman:1999:NPA:1624312.1624324}
\end{defi}

\begin{defi}[Induced single-controller-stochastic-game]
Let $M$ be an single-controller-stochastic-game, and let $L$ be any subset of $S$. The \term{induced single-controller-stochastic-game}, $M_L$ , has states $L\cup\accol{l_0}$, and transitions and state games as follows:
\begin{enumerate}
 \item The states in $M_L$ are associated with the same games as in $M$.
 \item The state $l_0$ is associated with a game where the adversary obtains the value $P_{\mbox{max}}$ for any joint action (a "worst case" state for the agent).
 \item For any action $a,\fun{P_{M_L}}{l_0,l_0,a_0}=1$.
 \item For any states $s,t\in L$, and $a\in A$, we have that $\fun{P_M}{s,t,a}=\fun{P_{M_L}}{s,t,a}$.
 \item For every $s\in L$, and $t\notin L$, and for every action $a\in A$, we have that $\fun{P_{M_L}}{s,t,a}=0$.
 \item For every $s\in L$, and $a\in A$, we have that $\fun{P_{M_L}}{s,l_0,a}=\displaystyle\sum_{j\notin L}{\fun{P_M}{s,t,a}}$.
\end{enumerate}
\cite{Brafman:1999:NPA:1624312.1624324}
\end{defi}


\section{Empirical Law Discovery}

\begin{defi}[$X$-of-$N$ representation]
Let \accol{A_i|1\leq i\leq\mbox{MaxAtt}} be the set of attributes of a domain, and for each $A_i$, \accol{V_{ij}|1\leq j\leq\mbox{MaxAttVal}_i} be its value set where $\mbox{MaxAtt}$ is the number of attributes, and $\mbox{MaxAttVal}_i$, is the number of different values of $A_i$. An \term{$X$-of-$N$ representation} is a set, denoted as $\mbox{$X$-of-}\accol{AV_k|AV_k\mbox{ is an attribute-value pair denoted as ``}A_i=V_{i,j}\mbox{'', }1\leq k\leq N_+, N\leq N_+, 1\leq N\leq\mbox{MaxAtt}}$, where $N_+$ is the number of attribute-value pairs in the $X$-of-$N$ representation, called the size of the $X$-of-$N$ representation, and $N$ is the number of different attributes that appear in the $X$-of-$N$ representation. The value of an $X$-of-$N$ can be any number between $0$ and $N$. Given an instance, its value is $X$ \iffTx{} $X$ of the $AV_k$ are true. An attribute-value pair $\fun{AV_k}{A_i=V_{ij}}$ is true for an instance \iffTx{} attribute $A_i$, of the instance has value $V_{ij}$.
\cite{conf/ijcai/Zheng95}
\end{defi}