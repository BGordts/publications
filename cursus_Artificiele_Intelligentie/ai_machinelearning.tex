%\section{Reinforcement Learning (Machine Learning)}
%\remarksimp{{\bf Noot:} In het academiejaar 2009-2010 werd de les rond Reinforcement Learning gegeven door middel van een gastcollege. Hierdoor kan dit misschien een optionele topic zijn, die in andere academiejaren vervangen wordt door een ander onderwerp.}
%\paragraph{}
%\termen{Reinforcement learning} is een andere implementatie van Machinelearning (zie \ref{s:conceptLearning} voor Concept Learning). Hierbij leert een bepaald algoritme door middel van interactie met zijn omgeving. Hierbij dient het te leren wat een goede oplossing/actie is in een bepaalde situatie. Nadat hij deze actie ondernomen heeft kan (eventueel na een bepaalde tijd) een beloning of een straf volgen. Op basis van deze beloning of straf, zal het algoritme zichzelf aanpassen en zo proberen om zichzelf te verbeteren. De oplossing wordt dus in principe nooit getoond. Aangezien het menselijk brein op eenzelfde manier werkt, is het logisch dat we proberen de technieken die de natuur hanteert na te bootsen. Bijgevolg is Re\"inforcement Learning niet alleen een studiegebied van de Artifici\"ele Intelligentie, maar ook van bijvoorbeeld de Psychologie en de Neurologie.
%\paragraph{}
%De Psychologie stelt \termen{Trail-and-Error} als \'e\'en van de methodes die mensen hanteren om een bepaald principe te leren. Het grootste probleem hierbij is dat het beloningsmechanisme meestal traag werkt: We worden niet altijd onmiddellijk beloond voor een goede actie, soms volgt de beloning veel later. Hoe weten we dan welke acties uiteindelijk tot deze beloning geleid hebben? Een ander probleem is dat we een mechanisme moeten hebben die na enige tijd de opgedane kennis, gaat exploiteren. We onderscheiden dus twee fasen: De \termen{exploratie-fase}, waarin we proberen om het principe te achterhalen en een goed beleid uit te bouwen. Gevolgd door de \termen{exploitatie-fase} waarbij we ons proberen te redden met de opgedane kennis. Meestal bevinden we ons nooit helemaal in \'e\'en van de twee fases. Ook wanneer we denken ons te kunnen redden in de wereld kan een tegenvaller ons weer terug meer doen verkennen, en ons minder op onze kennis doen laten rekenen.
%\paragraph{}We onderscheiden twee gevallen waarin we proberen te leren: Bij \termen{Supervised Learning} is het doel op voorhand gekend. We zoeken dus alleen een weg om dit doel te bereiken. Bij \termen{Unsupervised Learning} daarintegen is het doel op voorhand niet gekend, alleen door een beloningsmechanisme hebben we een indicatie of we op de goede weg zijn (Dit kan vergeleken worden met het Warm-Koud Concept die wordt gebruikt om naar een bepaald doel geleid te worden).
%\subsection{Abstract Model}
%Wat proberen we nu eigenlijk onze machine bij te brengen? We proberen de machine in een bepaalde staat (bijvoorbeeld de huidige positie van een robot) de juiste actie te laten ondernemen. Indien we dus een verzameling defini\"eren van alle staten waarin onze machine zich kan bevinden $S$, en we defini\"eren een verzameling van mogelijke acties $A$ dan defini\"eren we het \termen{Model} $M$ (niet te verwarren met het Model in Automatisch Redeneren) als een deelverzameling van het carthesisch product tussen $S$ en $A$ dan bevat $M\subset S\times A$ alle tupples die geldig zijn (dit betekent dus dat we de actie ook effectief kunnen uitvoeren in deze staat). Een subset van dit Model is de \termen{Value} $V\subset M$, deze verzameling bevat de tupples waarvan we uitgaan dat indien we in die staat, die actie ondernemen, we hiervoor een beloning zullen krijgen (dit is dus niet per definitie zo). De collecie van staten, en hun acties die dan wel een beloning ontvangen is de \termen{Reward} verzameling $R\subset V$. De uiteindelijke verzameling, de \termen{Policy} $ P\subset R$, is uiteindelijk een collectie van tupples waarbij iedere staat juist \'e\'en keer voorkomt, namelijk met de actie die we in dat geval \underline{zouden moeten} uitvoeren.
%\paragraph{}Het vorige model kan onterecht insinueren dat een staat of actie altijd een expressie naar de wereld dient te zijn. Dit is echter niet zo, ook bijvoorbeeld het interne geheugen van een robot kan mee opgenomen worden in de staat. Verder kan een actie bijvoorbeeld het veranderen van een stuk geheugen inhouden of het verplaatsen van de aandacht naar een bepaald aspect.
%\paragraph{}Hoe ziet Reinforcement Learning er dan uit vanuit het perspectief van de machine? We maken de tijd $t=0,1,2,\ldots$ discreet, waardoor we dus op bepaalde tijdstippen een keuze dienen te maken (deze tijdstippen kunnen in de echte wereld echter niet op vaste intervallen bepaald zijn). Hierbij observeert de machine op een bepaald tijdstip $t$ de huidige staat van het probleem $s_t\in S$. Vanuit deze staat zullen we vervolgens een actie produceren en ondernemen $a_t\in A\left(s_t\right)$. Dit resulteert vervolgens in een zekere beloning, een re\"eel getal: $r_{t+1}\in\Rr$. En in een nieuwe staat $s_{t+1}$, waarna de cyclus opnieuw begint. Hierbij is het de bedoeling dat we de keuze van actie telkens verder verfijnen gebaseerd op de reeds gekende $s_t$'s, $r_t$'s en $a_t$'s. De keuze welke actie we zullen ondernemen hangt dan ook af van een systeem de we de \termen{Instant Policy} $\Pi_t$ op stap $t$ noemen. $\Pi_t$ bepaalt echter niet zelf de keuze, maar het geeft een kansverdeling tussen de verschillende acties, zo is:
%\begin{equation}
%\Pi_t\left(s_t,a\right)=\mbox{waarschijnlijkheid dat $a_t=a$ bij een staat $s_t$}
%\end{equation}
%Het spreekt voor zich dat we bij aanvang iedere actie dezelfde kans geven om voor te vallen bij een bepaalde actie, naarmate we echter meer te weten komen, zullen de kansen convergeren zodat \'e\'en bepaalde actie de grootste kans krijgt terwijl andere acties na verloop van tijd, nooit meer gekozen zullen worden.
%\subsection{Beloningen en Doelen}
%Men kan zich hierbij de vraag stellen of een beloning in de vorm van \'e\'en re\"eel getal hierbij volstaat om na verloop van tijd de machine de notie van het doel te leren begrijpen. Anderzijds kan een beloning juist flexibeler zijn dan een doel. Een doel legt immers alleen uit wat we willen bereiken, terwijl een beloning zich meer richt op hoe we iets moeten bereiken. Het doel is immers iets wat buiten de machine's directe controle valt. Om toch een oplossing te vinden, moet de machine zijn tot dan toe bereikt succes ergens kunnen meten. Bovendien kunnen we het vinden van het doel ook opvatten als een optimalisatie opdracht voor de totale beloning.
%\paragraph{}Hoe berekenen we nu welke beloning een actie dient te krijgen. Hiervoor bestaan zoals gewoonlijk verschillende implementaties. Indien we een duidelijk onderscheid kunnen maken tussen verschillende delen in het verloop van het probleem, die min of meer van elkaar onafhankelijk zijn, is een \termen{Episodic task} een goede implementatie. Hierbij wordt na afloop van een episode, een min of meer onafhankelijk deel, een beloning berekend. Iedere actie krijgt vervolgens deze beloning uitgekeerd. Hierdoor kan echter het vreemde effect onstaan dat een actie ook beloond wordt voor overgangen die ervoor gebeurd zijn.
%\paragraph{}Omdat we er vanuit gaan dat een actie op een bepaald moment alleen maar effect heeft op de situatie die volgen (en dus niet op de situaties die ervoor kwamen), dienen we een expressie te vinden die de totale beloning voor een actie berekent. Zoals reeds eerder gesteld is deze niet gelijk aan de beloning die we onmiddelijk bekomen nadat we onze actie hebben ondernomen. Een techniek hiervoor heet \termen{Discount Return}. Hierbij beschouwen we dan ook de beloning voor een actie $a_t$ op tijdstip $t$ als:
%\begin{equation}
%R_t=\left.\displaystyle\sum_{i=1}^\infty{\gamma^{i-1}\cdot r_{t+k}}=r_{t+1}+\gamma\cdot r_{t+2}+\gamma^2\cdot r_{t+3}+\ldots\ \ \ \ \right|\gamma\in\left[0,1\right]
%\end{equation}
%$\gamma$ is een parameter die we de \termen{discount rate} noemen. Indien $\gamma$ dicht bij $0$ ligt, spreken we over een \termen{Shortsighted} Discount Rate. Indien $\gamma$ dicht bij $1$ ligt, spreken we over een \termen{Farsighted} Discount Rate. Meestal hangt het van het probleem in kwestie af hoe we de parameter kiezen. Problemen waarbij de acties meestal pas na een lange tijd effect hebben, geven we beter een grote Discount Rate. Problemen waarbij de effecten onmiddellijk zichtbaar zijn, zijn beter af met een kleine Discount Rate.
%\subsection{Evaluative Feedback}
%Het is de bedoeling dat we met de beloning, ons concept en dus bijgevolg onze Instant Policy aanpassen. Hiervoor bestaan enkele technieken, die allemaal onder de noemer \termen{Evaluative Feedback} vallen. Dit concept kunnen we vervolgens verder indelen. Zo bestaat er een verschil tussen \termen{Associative Evaluating Functions} waarbij het resultaat van een bepaalde acties, gevolgen heeft voor alle tuples in $M$ of \termen{Nonassiciative Evaluating Functions} waarbij we de invloed beperken tot slechts \'e\'en tuple. Bij dit laatste leren we slechts \'e\'en pad dat naar de oplossing leidt.