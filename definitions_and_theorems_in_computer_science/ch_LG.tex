\chapter{Machine Learning}

\begin{defi}[Learning algorithm for a meta-domain]%Problem: SxG??
Formally, an algorithm $A$ is a \term{learning algorithm for a meta-domain} $M$ in a hypothesis space $H$ with respect to a set of problem distributions $T$, if for any domain $D\in M$, any choice of a problem distribution $P$ in $T$, and any target problem solver $f\in H$,
\begin{enumerate}
\item $A$ takes as input the specification of a domain $D\in M$, an error parameter $E$, and a confidence parameter $\sigma$,
\item $A$ may call \alg{SolvedProblem}, which returns examples $\brak{x,\ffun{x}}$ for $D$, where $x$ is chosen with probability $\Pfun{x}$ from $S\times G$. The number of oracle calls of $A$ and its running time must be polynomial in the maximum problem size and the length of its input.
\item For all $D\in M$ and distributions $P\in T$, with probability at least $\brak{1-\delta}A$ outputs a program $f'$ that approximates $f$ in the sense that
\begin{equation}
\displaystyle\sum_{x\in\Delta}{\Pfun{x}}\leq\epsilon
\end{equation}
where $\Delta=\accol{x | f'\mbox{ fails on }x\mbox{ while }f\mbox{ succeeds}}$.
\item There is a polynomial $R$ such that, for a maximum problem size $n,\frac{1}{\epsilon},|frac{1}{\delta}$, maximum length $I$ and maximum step length $r$ of any solution output by \alg{SolvedProblem}, and an upper bound $t$ on the running times of programs in $D$ on inputs of size $n$, if $A$ outputs $f'$, the run time of $f'$ is bounded by $\fun{R}{n,l,r,t,\frac{1}{\epsilon},\frac{1}{\delta}}$
\end{enumerate}
\cite{conf/ijcai/Tadepalli91}
\end{defi}

\begin{defi}[Satisfying a spare solution space bias]
A problem solver $f$ for a domain $D$ and a problem distribution $P$ \term[Satisfying a sparse solution space bias]{satisfies a sparse solution space bias} if there is a set of operator sequences $m_f$ such that, on any problem $x\in D$ such that $\Pfun{x}>0$, $\ffun{x}\in m_f$ and $\abs{m_f}$ is bounded by a polynomial $Q$ in the problem size $n$.
\cite{conf/ijcai/Tadepalli91}
\end{defi}

\begin{defi}[Satisfying a macro table bias]
A problem solver $f$ \term[Satisfying a macro table bias]{satisfies a macro table bias} for a domain $D$ in $M$ if there is a feature ordering $O=\brak{l,\ldots,n}$ such that,
\begin{enumerate}
 \item $D$ is serially decomposable for $O$, and
 \item $f$ constructs all its solutions using a macro table $M$ as follows: for each feature $i$ from $1$ to $n$, macros $M_{j,i}$ are successively applied, where $j$ is the value of feature $i$ in the state before applying the macro.
\end{enumerate}
\cite{conf/ijcai/Tadepalli91}
\end{defi}

\begin{defi}[Beliefs in Conjoint Analysis]
We allow weighted beliefs with a weight parameter coming from \ocinterval{0}{1} where $1$ means full truth degree (complete certainty, the perfect belief), while a value $\alpha\in\oointerval{0}{1}$ describe a regular belief that can be doubted.
\begin{enumerate}
 \item \term{Regular belief}s such as:
 \begin{equation}
(\fun{A_1}{a_1}\wedge\ldots\wedge\fun{A_t}{a_t}):\alpha
 \end{equation}
 \item \term{Indifference belief}s such as:
 \begin{equation}
 \left(L\leftrightarrow R\right):1
 \end{equation}
 Indifference beliefs are always have full truth because we claim that if the respondent would distinguish degrees of truth then she is able to express preference.
 \item \term{Negative belief}s such as:
 \begin{equation}
 \left(\neg F\right):1
 \end{equation}
\end{enumerate}
where $A_i$ are attribute predicates and $L$, $R$, $F$ are regular atom conjunctions. Again, it is obvious in conjoint to don't ask user to express thoughts on negative information. As such there are no real negative beliefs such as $F:0$. Moreover, the reader may notice that we adopt the intuitionistic logic approach i.e., there is no assumption on any kind of law of excluded middle, as we don't necessarily assume $F:0\Leftrightarrow\left(\neg F\right):1$.
\cite{conf/fedcsis/GiurcaSB12}
\end{defi}

\begin{defi}[Single-controller-stochastic-game, stage game, agent, adversary, probabilistic transition function]
A \termabbrev{single-controller-stochastic-game}{SCSG} $M$ on states $S=\accol{1,2,\ldots,N}$ and actions $A=\accol{a_1,a_2,\ldots,a_k}$, consists of:
\begin{enumerate}
 \item \term[Stage game]{Stage games}: each state $s\in S$ is associated with a zero-sum game in strategic form, where the action set of each player is $A$. The first player is termed \term{agent} and the second player is termed \term{adversary}.
 \item \term{Probabilistic transition function}: $\fun{P_M}{s,t,a}$ is the probability of a transition from $s$ to $t$ given that the first player (termed agent), plays $a$.
\end{enumerate}
\cite{Brafman:1999:NPA:1624312.1624324}
\end{defi}

\begin{defi}[$\alpha$-approximation of a single-controller-stochastic-game]
Let $M$ and $\overline{M}$ be single-controller-stochastic-games over the same state space. We say that $\overline{M}$ is an \term[$\alpha$-approximation of a single-controller-stochastic-game]{$\alpha$-approximation of $M$} if for every state $s$ we have:
\begin{enumerate}
 \item If \fun{P_M}{s,t,a} and \fun{P_{\overline{M}}}{s,t,a} are the probabilities of transition to $t$ given that the action carried out by the agent is $a$, in $M$ and $\overline{M}$ respectively, then, $\fun{P_M}{s,t,a}-\alpha\leq \fun{P_{\overline{M}}}{s,t,a}\leq \fun{P_M}{s,t,a}+\alpha$
 \item The game associated with $s$ in $\overline{M}$ is the game associated with it in $M$ restricted to a non-empty subset of the columns.
\end{enumerate}
\cite{Brafman:1999:NPA:1624312.1624324}
\end{defi}

\begin{defi}[Induced single-controller-stochastic-game]
Let $M$ be an single-controller-stochastic-game, and let $L$ be any subset of $S$. The \term{induced single-controller-stochastic-game}, $M_L$ , has states $L\cup\accol{l_0}$, and transitions and state games as follows:
\begin{enumerate}
 \item The states in $M_L$ are associated with the same games as in $M$.
 \item The state $l_0$ is associated with a game where the adversary obtains the value $P_{\mbox{max}}$ for any joint action (a "worst case" state for the agent).
 \item For any action $a,\fun{P_{M_L}}{l_0,l_0,a_0}=1$.
 \item For any states $s,t\in L$, and $a\in A$, we have that $\fun{P_M}{s,t,a}=\fun{P_{M_L}}{s,t,a}$.
 \item For every $s\in L$, and $t\notin L$, and for every action $a\in A$, we have that $\fun{P_{M_L}}{s,t,a}=0$.
 \item For every $s\in L$, and $a\in A$, we have that $\fun{P_{M_L}}{s,l_0,a}=\displaystyle\sum_{j\notin L}{\fun{P_M}{s,t,a}}$.
\end{enumerate}
\cite{Brafman:1999:NPA:1624312.1624324}
\end{defi}


\section{Empirical Law Discovery}

\begin{defi}[$X$-of-$N$ representation]
Let \accol{A_i|1\leq i\leq\mbox{MaxAtt}} be the set of attributes of a domain, and for each $A_i$, \accol{V_{ij}|1\leq j\leq\mbox{MaxAttVal}_i} be its value set where $\mbox{MaxAtt}$ is the number of attributes, and $\mbox{MaxAttVal}_i$, is the number of different values of $A_i$. An \term{$X$-of-$N$ representation} is a set, denoted as $\mbox{$X$-of-}\accol{AV_k|AV_k\mbox{ is an attribute-value pair denoted as ``}A_i=V_{i,j}\mbox{'', }1\leq k\leq N_+, N\leq N_+, 1\leq N\leq\mbox{MaxAtt}}$, where $N_+$ is the number of attribute-value pairs in the $X$-of-$N$ representation, called the size of the $X$-of-$N$ representation, and $N$ is the number of different attributes that appear in the $X$-of-$N$ representation. The value of an $X$-of-$N$ can be any number between $0$ and $N$. Given an instance, its value is $X$ \iffTx{} $X$ of the $AV_k$ are true. An attribute-value pair $\fun{AV_k}{A_i=V_{ij}}$ is true for an instance \iffTx{} attribute $A_i$, of the instance has value $V_{ij}$.
\cite{conf/ijcai/Zheng95}
\end{defi}

\section{Reinforcement Learning}

\begin{defi}[$\mu$-approximation $T$-step planning algorithm for a Dynamic Bayesian Network-Markov Decision Processes]
A \term{$\mu$-approximation $T$-step planning algorithm for a Dynamic Bayesian Network-Markov Decision Processes} is one that, given a Dynamic Bayesian Network-Markov Decision Processes, produces a (compactly represented) policy n such that $\fun{U_M^{\pi}}{x,T}\geq\brak{1-\mu}\fun{U_M^{\star}}{x,T}$.
\cite{conf/ijcai/KearnsK99}
\end{defi}

\begin{defi}[induced Dynamic Bayesian Network-Markov Decision Processes on a subset of the transition components of a Dynamic Bayesian Network-Markov Decision Processes]
Let $M$ be a Dynamic Bayesian Network-Markov Decision Processes and let $T$ be any subset of the transition components in the model. The \term[induced Dynamic Bayesian Network-Markov Decision Processes on a subset of the transition components of a Dynamic Bayesian Network-Markov Decision Processes]{induced Dynamic Bayesian Network-Markov Decision Processes on $T$}, denoted $M_T$, is defined as follows:
\begin{enumerate}
 \item $M_T$ has the same set of sate variables as $M$; however in $M_T$ each variable $X_i$ has, in addition to its original set of values \fun{\mbox{Val}^M}{X_i}, a new value $w$.
 \item $M_T$ has the same transition graphs as $M$. For each $a$, $i$ and $u\in\fun{\mbox{Val}^M}{\fun{\mbox{Pa}_a}{X'_i}}$, we have that $\fun{P_a^{M_T}}{X_i'|u}=\fun{P_a^M}{X_i'|u}$ if the corresponding transition component is in $T$; in all other cases, $\fun{P_a^{M_T}}{w|u}=1$ and $\fun{P_a^{M_T}}{x_i|u}=0$ for all $x_i\in\fun{\mbox{Val}^M}{X_i}$.
 \item $M_T$ has the same set $\calR$ as $M$. For each $i=1,2,\ldots,k$ and $c\in\fun{\mbox{Val}^M}{C_i}$, we have that $\fun{R_i^{M_T}}{c}=\fun{R_i^M}{c}$. For other vectors $c$, we have that $\fun{R_i^{M_T}}{c}=-R_{\mbox{max}}$.
\end{enumerate}
\cite{conf/ijcai/KearnsK99}
\end{defi}

\begin{defi}[$\epsilon$-mixed Markov chain at a certain moment in time]
Let $Q$ be a transition model for a Markov chain and let $\accol{X^{\brak{t}}}_{t=0}^{\infty}$ represent the state of the chain. Let $S=\accol{x_1,x_2,\ldots,x_s}$. Let $\mu_j$ be the stationary probability of $x_j$ in the Markov chain. We say that Markov chain $Q$ is \term[$\epsilon$-mixed Markov chain at a certain moment in time]{$\epsilon$-mixed at time $m$} if $\max_{i,j}\abs{\fun{P}{X^{\brak{t}}=x_j|X^{\brak{1}}=x_i}-\mu_j}\leq\epsilon$.
\cite{conf/ijcai/KearnsK99}
\end{defi}

\begin{defi}[Dependency graph for the Dynamic Bayesian Network]
Consider a Dynamic Bayesian Network over the state variables $X_1,X_2,\ldots,X_n$ .The \term[Dependency graph for the Dynamic Bayesian Network]{dependency graph $V$ for the Dynamic Bayesian Network} is a directed cyclic graph whose nodes are $X_1,X_2,\ldots,X_n$ and where there is a directed edge from $X_i$ to $X_j$ if there is an edge in the transition graph of the Dynamic Bayesian Network from $X_i'$ to $X_j'$.
\cite{conf/ijcai/KearnsK99}
\end{defi}