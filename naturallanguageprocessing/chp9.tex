\section{Named Entity Recognition and Semantic Role Labeling}
\begin{df}{Named entity recognition}
\sb{} is a process where one recognizes and classifies named expressions in text (such as persons, companies, locations, protein names,...). For instance \stc{\underline{John Smith} works for \underline{IBM}.}. Two problems in this context are \flv{Segmentation} and \flv{Classification}.
\end{df}
\begin{df}{Segmentation}
sb{} is a process where one tries to find segments in a sequence which satisfy some constraint. In other words one determines which constituents in a sentence are semantic arguments. Since these constraints can be rather complex and fuzzy, \sb{} is sometimes a problem.
\end{df}
\begin{df}[Recognition]{Classification}
sb{} is a process where tries to determine the role for each of the segments found by the \flv{Segmentation} in the sentence.
\end{df}
\begin{df}[Thematic role labeling, Case role assignment, Shallow semantic parsing]{Semantic role labeling}
\sb{} is a process where one recognizes the basic event structure of a sentence (like \stc{who?}, \stc{does what?}, \stc{to whom/what?}, \stc{when?}, \stc{where?}). Each algorithm uses the same high level structure as \algref{semanticrolelabel}. For each node a vector is built. A classification model generated by a training set will then return semantical content about these features. Most algorithm prune nodes according to some set of rules before classification. Furthermore some classification models use \flv{Chunking} instead of parsing. Usually one evaluates the performance of such algorithms by the \flv{Precision}, \flv{Recall} and \flv{F-measure}. The classification algorithm will always generate a \flv{Frame}. Therefore \sb{} is a form of \flv{Relational learning} and \flv{Context dependent classification}.
\end{df}
\importalgo{semanticrolelabel}{A high level description of a \flv{Semantic role labeling} algorithm.}
\begin{df}{Semantic frame labeling}
\sb{} is an extension of \flv{Semantic role labeling} where one does not only label the individual roles but the entire frame structure. This might involve the analysis of several sentences and disambiguation of the meaning of the predicate.
\end{df}
\begin{df}[ProBank]{Proposition bank}
A \sb{} is a resource of sentences annotated with semantic roles. These roles differ per language. The semantic roles are encoded in the for of numbers specific to the verb sense. For instance the \stc{agree} relation: \funm{agree}{\mbox{agreer},\mbox{object of the agreement},\mbox{other entity agreeing}}.
\end{df}
\begin{df}{Frame}
A \sb{} is a script-like structure that instantiates a set of frame-specific semantic rules called \flv{Frame elements}. It assigns the \flv{Core-role}s and \flv{Non-core role}s across the different verbs. Also relations like causality are expressed.
\end{df}
\begin{df}{FrameNet}
A \sb{} is a resource containing a large number of \flv{Frame} elements as roles.
\end{df}
\begin{df}{Classifier}
A \sb{} is an algorithm that based on a vector $\vec{x}$ returns a label $y$. \sb{}s come in two flavors: \flv{Generative classifier}s and \flv{Discriminative classifier}s.
\end{df}
\begin{df}{Generative classifier}
A \sb{} is an algorithm that learns a model of the joint probability \Prob{\vec{x},y} and makes its predictions by using \flv{Bayes rule} to calculate \Prob{y|\vec{x}} and then selects the most likely label. Examples of such classifiers are \flv{Naive Bayes} and \flv{Hidden Markov Model}s.
\end{df}
\begin{df}{Discriminative classifier}
A \sb{} is an algorithm trained to model the conditional probability \Prob{y|\vec{x}} directly and selects the most likely label $y$ or learns a direct map from inputs $\vec{x}$ to the class labels $y$. Examples include \flv{Maximum entropy model}s, \flv{Support vector machine}s, ...
\end{df}
\begin{df}{Linear regression}
\sb{} is a model where we assume that the classes that we want to find are linearly separable. Therefore the result is a linear recombination of the elements in the input vector. Thus:
\begin{equation}
y\approx w_0+\displaystyle\sum_{i=1}^{n}w_i\cdot x_i
\end{equation}
In most cases $w_0$ is assumed to be zero. The weights are learned by minimizing the cost of a \flv{Training set} with:
\begin{equation}
\funm{cost}{\vec{w},\vec{\vec{x}},\vec{y}}=\displaystyle\sum_{i=1}^{m}{\brak{w_0-y_i+\displaystyle\sum_{j=1}^{n}w_i\cdot x_{j\,i}}}^2
\end{equation}
If one performs classification, the expected answer is calculated and the system answers with right or wrong. One could train this model to output Boolean values by using $0$ and $1$ in the test-output but the calculated results are not guaranteed to lie in the interval $\fbrk{0,1}$. Therefore one uses \flv{Logistic regression}.
\end{df}
\begin{df}{Logistic regression}
\sb{} is a model where one assumes that the \flv{Log odd function} of probability of a certain vector $\vec{x}$ being classified as true, can be modeled with \flv{Linear regression}. Therefore it guarantees that the probabilities are elements of the $\fbrk{0,1}$ interval. More formally one states:
\begin{equation}
\fun{\ln}{\displaystyle\frac{\Prob{y=\truet{}|\vec{x}}}{1-\Prob{y=\truet{}|\vec{x}}}}\approx w_0+\displaystyle\sum_{i=1}^{n}w_i\cdot x_i
\end{equation}
One than classifies a vector $\vec{x}$ as true if:
\begin{equation}
w_0+\displaystyle\sum_{i=1}^{n}w_i\cdot x_i>0
\end{equation}
Learning the weights is done through convex optimization. Therefore several method are invented like \flv{Gradient ascent}, \flv{Conjugate gradient}, ...
\end{df}
\begin{df}[Logit]{Log odds function}
The \sb{} is defined as follows:
\begin{equation}
\funm{logit}{\fun{P}{x}}=\fun{\ln}{\displaystyle\frac{\fun{P}{x}}{1-\fun{P}{x}}}
\end{equation}
\end{df}
\begin{df}[Maxent, Multinominial logistic regression]{Maximum entropy model}
The \sb{} is a model that is a generalization of \flv{Logistic regression}. Instead of two classes, different classes $y_1,y_2,\ldots,y_C$ are considered. Therefore. Given $N$ \flv{Feature function}s are provided
\end{df}
\begin{df}[MEMM]{Maximum entropy Markov model}

\end{df}
\begin{df}[CRF]{Conditional random field}

\end{df}