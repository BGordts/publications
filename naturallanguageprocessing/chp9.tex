\section{Named Entity Recognition and Semantic Role Labeling}
\begin{df}{Named entity recognition}
\sb{} is a process where one recognizes and classifies named expressions in text (such as persons, companies, locations, protein names,...). For instance \stc{\underline{John Smith} works for \underline{IBM}.}. Two problems in this context are \flv{Segmentation} and \flv{Classification}.
\end{df}
\begin{df}{Segmentation}
sb{} is a process where one tries to find segments in a sequence which satisfy some constraint. In other words one determines which constituents in a sentence are semantic arguments. Since these constraints can be rather complex and fuzzy, \sb{} is sometimes a problem.
\end{df}
\begin{df}[Recognition]{Classification}
sb{} is a process where tries to determine the role for each of the segments found by the \flv{Segmentation} in the sentence.
\end{df}
\begin{df}[Thematic role labeling, Case role assignment, Shallow semantic parsing]{Semantic role labeling}
\sb{} is a process where one recognizes the basic event structure of a sentence (like \stc{who?}, \stc{does what?}, \stc{to whom/what?}, \stc{when?}, \stc{where?}). Each algorithm uses the same high level structure as \algref{semanticrolelabel}. For each node a vector is built. A classification model generated by a training set will then return semantical content about these features. Most algorithm prune nodes according to some set of rules before classification. Furthermore some classification models use \flv{Chunking} instead of parsing. Usually one evaluates the performance of such algorithms by the \flv{Precision}, \flv{Recall} and \flv{F-measure}. The classification algorithm will always generate a \flv{Frame}. Therefore \sb{} is a form of \flv{Relational learning} and \flv{Context dependent classification}.
\end{df}
\importalgo{semanticrolelabel}{A high level description of a \flv{Semantic role labeling} algorithm.}
\begin{df}{Semantic frame labeling}
\sb{} is an extension of \flv{Semantic role labeling} where one does not only label the individual roles but the entire frame structure. This might involve the analysis of several sentences and disambiguation of the meaning of the predicate.
\end{df}
\begin{df}[ProBank]{Proposition bank}
A \sb{} is a resource of sentences annotated with semantic roles. These roles differ per language. The semantic roles are encoded in the for of numbers specific to the verb sense. For instance the \stc{agree} relation: \funm{agree}{\mbox{agreer},\mbox{object of the agreement},\mbox{other entity agreeing}}.
\end{df}
\begin{df}{Frame}
A \sb{} is a script-like structure that instantiates a set of frame-specific semantic rules called \flv{Frame elements}. It assigns the \flv{Core-role}s and \flv{Non-core role}s across the different verbs. Also relations like causality are expressed.
\end{df}
\begin{df}{FrameNet}
A \sb{} is a resource containing a large number of \flv{Frame} elements as roles.
\end{df}
\begin{df}{Classifier}
A \sb{} is an algorithm that based on a vector $\vec{x}$ returns a label $y$. \sb{}s come in two flavors: \flv{Generative classifier}s and \flv{Discriminative classifier}s.
\end{df}
\begin{df}{Generative classifier}
A \sb{} is an algorithm that learns a model of the joint probability \Prob{\vec{x},y} and makes its predictions by using \flv{Bayes rule} to calculate \Prob{y|\vec{x}} and then selects the most likely label. Examples of such classifiers are \flv{Naive Bayes} and \flv{Hidden Markov Model}s.
\end{df}
\begin{df}{Discriminative classifier}
A \sb{} is an algorithm trained to model the conditional probability \Prob{y|\vec{x}} directly and selects the most likely label $y$ or learns a direct map from inputs $\vec{x}$ to the class labels $y$. Examples include \flv{Maximum entropy model}s, \flv{Support vector machine}s, ...
\end{df}
\begin{df}{Linear regression}
\sb{} is a model where we assume that the classes that we want to find are linearly separable. Therefore the result is a linear recombination of the elements in the input vector. Thus:
\begin{equation}
y\approx w_0+\displaystyle\sum_{i=1}^{n}w_i\cdot x_i
\end{equation}
In most cases $w_0$ is assumed to be zero. The weights are learned by minimizing the cost of a \flv{Training set} with:
\begin{equation}
\funm{cost}{\vec{w},\vec{\vec{x}},\vec{y}}=\displaystyle\sum_{i=1}^{m}{\brak{w_0-y_i+\displaystyle\sum_{j=1}^{n}w_i\cdot x_{j\,i}}}^2
\end{equation}
If one performs classification, the expected answer is calculated and the system answers with right or wrong. One could train this model to output Boolean values by using $0$ and $1$ in the test-output but the calculated results are not guaranteed to lie in the interval $\fbrk{0,1}$. Therefore one uses \flv{Logistic regression}.
\end{df}
\begin{df}{Logistic regression}
\sb{} is a model where one assumes that the \flv{Log odd function} of probability of a certain vector $\vec{x}$ being classified as true, can be modeled with \flv{Linear regression}. Therefore it guarantees that the probabilities are elements of the $\fbrk{0,1}$ interval. More formally one states:
\begin{equation}
\fun{\ln}{\displaystyle\frac{\Prob{y=\truet{}|\vec{x}}}{1-\Prob{y=\truet{}|\vec{x}}}}\approx w_0+\displaystyle\sum_{i=1}^{n}w_i\cdot x_i
\end{equation}
One than classifies a vector $\vec{x}$ as true if:
\begin{equation}
w_0+\displaystyle\sum_{i=1}^{n}w_i\cdot x_i>0
\end{equation}
Learning the weights is done through convex optimization. Therefore several method are invented like \flv{Gradient ascent}, \flv{Conjugate gradient}, ...
\end{df}
\begin{df}[Logit]{Log odds function}
The \sb{} is defined as follows:
\begin{equation}
\funm{logit}{\fun{P}{x}}=\fun{\ln}{\displaystyle\frac{\fun{P}{x}}{1-\fun{P}{x}}}
\end{equation}
\end{df}
\begin{df}[Maxent, Multinominial logistic regression]{Maximum entropy model}
The \sb{} is a model that is a generalization of \flv{Logistic regression}. Instead of two classes, different classes $y_1,y_2,\ldots,y_C$ are considered. Therefore. Given $N$ \flv{Feature function}s $f_i$ are provided, the probability of a class is given by:
\begin{equation}
\Prob{y|\vec{x}}=\displaystyle\frac{\fun{\exp}{\displaystyle\sum_{i=0}^N{w_i\cdot\fun{f_i}{y,\vec{x}}}}}{\displaystyle\sum_{y'\in C}\fun{\exp}{\displaystyle\sum_{i=0}^N{w_i\cdot\fun{f_i}{y,\vec{x}}}}}
\end{equation}
When the classes are mutually exclusive, the highest probability is chosen, otherwise one builds a separate classifiers for each class. Learning the weights is done through convex optimization. Therefore several method are invented like \flv{Gradient ascent}, \flv{Conjugate gradient}, ...
\end{df}
\begin{df}{Feature function}
A \sb{} is a function of the form:
\begin{equation}
\fun{f_i}{y,\vec{x}}=\guard{
1&\mbox{if $\tupl{y,\vec{x}}$ satisfies some constraint}\\
0&\mbox{otherwise}}
\end{equation}
\end{df}
\begin{df}{Maximum entropy principle}
The \sb{} states that when we make influences based on incomplete information, we should draw them from that probability distribution that has the maximum entropy permitted by the information we have. Berger showed that the exponential model for multinominal logistic regression when trained according to the maximum likelihood criterion, also finds the maximal entropy distribution.
\end{df}
\begin{df}{Entropy}
The entropy of a distribution is defined as:
\begin{equation}
\funm{entropy}{p}=-\displaystyle\sum_{x}\fun{p}{x}\cdot\fun{\log_2}{\fun{p}{x}}
\end{equation}
\end{df}
\begin{df}{Context-dependent classification}
Context-dependent classification is a form of classification that should satisfy the following condition: The class to which a feature vector is assigned depends on:
\begin{itemize}
 \item The feature vector itself;
 \item The values of other feature vectors;
 \item The existing relation among various classes.
\end{itemize}
When processing text, many variables are interdependent. When we classify left-to-right, for each word making a hard classification and using the classification outcome as a feature in the next classification, but we can also take into acount the dependencies when training a sequential classifier. For instance a \flv{Hidden Markov model}.
\end{df}
\begin{df}[MEMM]{Maximum entropy Markov model}
A \sb{} is the augmentation of the \flv{Maximum entropy model} so it can be applied to assign a class to each element of a sequence. In order to estimate the individual probability, of a transition from state $y'$ to a state $y$ producing an observation $\vec{x}$, we built a \flv{Maximum entropy model} as follows:
\begin{equation}
\Prob{y|y',\vec{x}}=\displaystyle\frac{\fun{\exp}{\displaystyle\sum_{i=1}^n w_i\cdot\fun{f_i}{\vec{x},y}}}{Z}
\end{equation}
One decodes a \sb{} using the \flv{Viterbi algorithm}. The training is done using \flv{Logistic Regression} and the \flv{Maximum entropy model}. To maximize the likelihood, one can use a \flv{Expectation-Maximization algorithm}. The \flv{Linear-chain conditional random field} is an improvement of this model.
\end{df}
\begin{df}[CRF]{Conditional random field}

\end{df}