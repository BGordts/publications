\section{Computational Lexical Semantics}
\begin{df}{Word sense}
The \sb{} of a word is the \flv{Lexical meaning}: the meaning defined by the origin and the usage of the word. This might vary according to the context. Such knowledge is stored in dictionaries.
\end{df}
\begin{df}{Homonymy}
\sb{} means a word has two rather different meanings. For instance \stc{bark}.
\end{df}
\begin{df}{Plysemy}
\sb{} means a word has two somehow rather meanings. For instance \stc{opening}.
\end{df}
\begin{df}{Metonymy}
\sb{} is the use of one aspect of a concept or entity to refer to other aspects of the entity or to the entity itself. For instance \stc{White House} refers to the administration whose office is in the White House.
\end{df}
\begin{df}{Synonymy}
\sb{} means two words have the same propositional meaning: they are substitutable by one another without changing the truth conditions of the sentence. For instance \stc{vermin} and \stc{pests}. Many words are near synonyms: they slightly differ in meaning.
\end{df}
\begin{df}{Antonymy}
\sb{} means two words have the opposite meaning. For instance \stc{cold} and \stc{hot}.
\end{df}
\begin{df}[Superordinate]{Hypernymy}
\sb{} means a word is more general than another word. For instance \stc{fruit} is a \flv{Hypernym} of \stc{apple}. This property can be defined in terms of entailment: sense $A$ is a hypernym of sense $B$ if $\forall x:\fun{A}{x}\Leftarrow\fun{B}{x}$. These relations are usually transitive.
\end{df}
\begin{df}[Subordinate]{Hyponymy}
\sb{} means a word is more specific than another word. For instance \stc{apple} is a \flv{Hyponym} of \stc{fruit}. This property can be defined in terms of entailment: sense $A$ is a hyponym of sense $B$ if $\forall x:\fun{A}{x}\Rightarrow\fun{B}{x}$. These relations are usually transitive.
\end{df}
\begin{df}{Meronymy}
\sb{} means a word is a part of another word. For instance \stc{wheel} is a \flv{Meronym} of a \stc{car}.
\end{df}
\begin{df}{Holonymy}
\sb{} means a word is the whole of another word. For instance \stc{wheel} is a \flv{Holonym} of a \stc{car}.
\end{df}
\begin{df}{WordNet}
\sb{} is a lexical database containing \flv{Noun}s, \flv{Verb}s, \flv{Adjective}s and \flv{Adverb}s in English together with their senses. Each of these words has different \flv{Sense}s and each of these senses contains a \flv{Gloss}: a dictionary style definition and a \flv{Synset}: a set of (near) synonyms for the sense. Sometimes usage examples are provided. Besides meanings, \sb{} also defines relations between words.
\end{df}
\begin{df}{Word sense disambiguation}
\sb{} is a process where one tries to select the correct sense of each word in a discourse. In order to do this, the correct context is needed. Formally one has a word $w$ with senses $s_1,s_2,\ldots s_n$. Furthermore a corpus has a number of contexts $c_1,c_2,\ldots,c_m$ and one can uses word $v_1,v_2,\ldots,v_p$ as contextual features for disambiguation. The processes usually relies on machine learning techniques. Usually these systems rely on two assumptions: \flv{One sense per discourse} and \flv{One sense per collocation}. The context is encoded using \flv{Bag-of-word feature}s and \flv{Collocational feature}s. The process can be carried out using resources like \flv{WordNet}, or by using a large corpus. The baseline for each \sb{} algorithm is the same. For each word, the given dictionary contains \flv{Semantic categorie}s (also known as \flv{Subject code}s) for each word sense. The score for a certain sense is the number of words in context $c_i$ that are compatible with the subject code of sense $s_k$. The algorithm will select the sense with the maximum score. Formally the score is calculated by the following formula:
\begin{equation}
\funm{score}{s_k}=\displaystyle\sum_{v_j\in c_i}\krdelta{\fun{t}{s_k},v_j}
\end{equation}
A typical algorithm to do this is the \flv{Simplified Lesk algorithm}. In other cases supervised learning is used, like for instance the \flv{Yarowski algorithm}.
\end{df}
\begin{tm}{One sense per discourse}
The \sb{}-assumption assumes that the sense of the target word is highly consistent within a document.
\end{tm}
\begin{tm}{One sense per collocation}
The \sb{}-assumption assumes that nearby words provide strong and consistent clues to the sense of a target word, conditional on relative distance, order and syntactical relationship.
\end{tm}
\begin{df}{Bag-of-word feature}
A \sb{} means one assigns a binary value to each word of the vocabulary encoding whether or not a word is present in the context.
\end{df}
\begin{df}{Collocational feature}
A \sb{} means one encodes information about specific positions for a certain scope around the target word, this information for instance consists out of the word and its \flv{Part-of-Speech tag}.
\end{df}
\begin{df}{Simplified Lesk algorithm}
The \sb{} is an algorithm used for \flv{Word sense disambiguation}. It counts the number of words in a certain context and returns the context with the most votes. A full listing is presented in \algref{simplifiedlesk}.
\end{df}
\importalgo{simplifiedlesk}{The Simplified Lesk algorithm}
\begin{df}{Yarowski algorithm}
The \sb{} is an algorithm used for \flv{Word sense disambiguation}. For each ambiguous word $w$ it learns a dictionary of collocations. It uses two sets for each sense $s_k$ of the ambiguous word $w$:
\begin{description}
 \item [$F_k$] The \flv{Set of collocations}: this set contains rules who describe a context. Each collections has a score that corresponds to the likelihood that the collection is specific to the sense $s_k$.
 \item [$E_k$] The \flv{Set of contexts}: The different contexts of the ambiguous word $w$ with sense $s_k$.
\end{description}
The algorithm is initialized with the corpus and collects all the contexts of the ambiguous word in the corpus. Next it filters out the senses below a certain threshold $\alpha$. A full listing is presented in \algref{yarowski}.
\end{df}
\importalgo{yarowski}{The Yarowski algorithm}
\begin{df}{Unsupervised word sense disambiguation}
The \sb{} is an algorithm used for \flv{Word sense disambiguation}. This method is quite popular since labeling is expensive. One does not labels based on human-readable senses but clusters texts according to some ``meaning'' with a \flv{Cluster algorithm}. Therefore a word is represented by a \flv{Feature vector} $w=\vec{f}$. Such system is trained by computing a context vector $\vec{c}$. Therefore each cluster defines a sense for a word. In order to cluster new words, \flv{Vector centroid}s are computed for each cluster. The context vector is calculated using the following formula:
\begin{equation}
\vec{c}=\displaystyle\frac{\displaystyle\sum_{i=1}^{n}\vec{x}_i}{n}
\end{equation}
With $x_i$ the feature vector of the context word $w_i$. Words in a new text are clustered by the sense which \flv{Vector centroid} is the closest.
\end{df}
\begin{df}{Selectional preferences}
\sb{} are a set of rules who determine which words are combinable for a given semantical role. For instance the sentence \stc{I eat gold for lunch.} doesn't make any sense. These restrictions are not rigid but express some preferences (and are thus executed by a probabilistic framework). A popular implementation is \flv{Resnik's model} of selection association: \flv{Selectional preference strength}. The system computes the difference in information a verb $v$ expresses about a class and the information the class itself expresses. The difference is calculated using the \flv{Kullback-Leibler} metric:
\begin{equation}
\fun{S_R}{v}=\displaystyle\sum_c\Prob{c|v}\cdot\fun{\log}{\displaystyle\frac{\Prob{c|v}}{\Prob{c}}}
\end{equation}
The specific preference of a verb in a class is calculated by the \flv{Selectional association}. The probabilites can be estimated by couning them from the corpus.
\end{df}
\begin{df}[Relative entropy]{Kullback-Leibler}
The \sb{} difference is a measurement of the difference between two distributions $P$ and $Q$. The \sb{} difference is defined as:
\begin{equation}
\fun{\Delta}{P,Q}=\displaystyle\sum_x\fun{P}{x}\cdot\fun{\log}{\displaystyle\frac{\fun{P}{x}}{\fun{P}{y}}}
\end{equation}
\end{df}
\begin{df}{Selectional association}
The \sb{} is a matric who measures the relative contribution of that class to the general selectional preference of the verb.
\begin{equation}
\fun{A_R}{v,c}=\displaystyle\frac{\Prob{c|v}}{\fun{S_R}{v}}\cdot\fun{\log}{\displaystyle\frac{\Prob{c|v}}{\Prob{c}}}
\end{equation}
\end{df}
\begin{df}{Word similarity}
\sb{} is a metric defined as the inverse \flv{Semantic distance}. One calculates the \sb{} by looking at the number of features two words share. Algorithm who calculate this metric are the \flv{Thesaurus based word similarity} or the \flv{Corpus based word similarity} systems.
\end{df}
\begin{df}{Thesaurus based word similarity}
The \sb{} is a way to calculate the word similarity. Therefore the algorithm stores a semantic network with for instance a \flv{Hypernym} hierarchy. The similarity is calculated based on the \flv{Path-length based similarity}:
\begin{equation}
\funm[path]{similarity}{c_1,c_2}=-\fun{\log}{\funm{pathlength}{c_1,c_2}}
\end{equation}
Where \funm{pathlength}{c_1,c_2} is the number of edges in the shortest path in the Thesaurus graph between the senses nodes $c_1$ and $c_2$. The \flv{Word similarity} is then defined as:
\begin{equation}
\funm{wordsim}{w_1,w_2}=\fun{\max_{c_1\in\funm{senses}{w_1},c_2\in\funm{senses}{w_2}}}{\funm{sim}{c_1,c_2}}
\end{equation}
\end{df}