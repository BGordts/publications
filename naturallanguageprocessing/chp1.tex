\section{Introduction}
\begin{df}{Natural language}A natural language is a language spoken and/or written by people.
\end{df}
\begin{df}{Artificial language}An artificial language is a language used for programming languages or constructed languages like Esperanto or Klingon.
\end{df}
\begin{df}{\nlp{}}\nlp{} is the ability of a machine tho process natural language and thus understand, generate or communicate with such language.
\end{df}
\begin{df}{Human language technology}Human language technology is a research branch interested in the technological aspects of \nlp{}.
\end{df}
\begin{df}{Computational linguistics}Computational language is a research domain where one focuses on the theory behind language (linguistics).
\end{df}
\begin{df}{Ambiguity}Ambiguity is the ability of a string to be interpreted in several ways. In \nlp{}, ambiguity comes in two flavors: part-of-speech and word sense.
\end{df}
\begin{df}{Part-of-Speech tagging}Part of speech tagging is a form of disambiguation where one given a word classifies this word as a non, verb,...
\end{df}
\begin{df}{Word sense disambiguation}Word sense disambiguation is a form of disambiguation between different meanings for the same word. Their are three dimensions in word sense disambiguation: \pnc{}, spelling and meaning.
\end{df}
\begin{df}{Heterographs}Two words are heterographs if the have a different spelling and meaning.
\end{df}
\begin{df}{Heteronyms}Two words are heteronyms if the have a different \pnc{} and meaning.
\end{df}
\begin{df}{Homonyms}Two words are homonyms if they have a different meaning but share the spelling and \pnc{}.
\end{df}
\begin{df}{Synonyms}Two words are synonyms if they have a different spelling but share the meaning.
\end{df}
\begin{df}{Phonetics}Phonetics is a \alnlp{} handling articulatory and acoustic properties of sounds.
\end{df}
\begin{df}{Phonology}\sb{} is a \alnlp{} handling the systematic use of sound to encode meaning.
\end{df}
\begin{df}{International Phonetic Alphabet (IPA)}The \sb{} is an alphabet containing the various symbols to express \pnc{}.
\end{df}
\begin{df}{Morphology}\sb{} is a \alnlp{} handling about the structure of a word. Words are split up in different basic components and one can analyze the appropriate \pos{}.
\end{df}
\begin{df}{Compounding}A \sb{} is a \mpcl{} rule where a lexeme that consists of more than one stem. \sb{} are very common in Dutch and German. \sb{} can be applied recursively.
\end{df}
\begin{df}{Derivation}A \sb{} is a \mpcl{} rule where an affix forms a new word from an existing word. For instance ``happi-ness''. \sb{} can be applied recursively.
\end{df}
\begin{df}{Inflection}An inflection is a modification of a word to express different grammatical properties. For instance ``work-s''.
\end{df}
\begin{df}{Syntax}\sb{} is a \alnlp{} where structural relations between words are analyzed. Syntax is also known as grammar.
\end{df}
\begin{df}{Semantics}\sb{} is a \alnlp{} where the meaning of the different words are analyzed.
\end{df}
\begin{df}{Pragmatics}\sb{} is a \alnlp{} where the relationship of the meaning to the goals of the speaker are analyzed. For instance verbatim or ironic, question or statement,...
\end{df}
\begin{df}{Discourse}\sb{} is a \alnlp{} where the relations between sentences are analyzed. For instance to who refers \stc{her} in the sentence \stc{I made her duck.}.
\end{df}
\begin{df}{Information retrieval}\sb{} is an \app{} of \nlp{} where one tries to retrieve information by searching for documents, information within document, metadata, relational databases,...
\end{df}
\begin{df}{Machine translation (MT)}\sb{} is an \app{} of \nlp{} where a machine tries to translate a fragment from one language into another.
\end{df}
\begin{df}{Dialogue System}A \sb{} is a system where the user performs a dialogue with a machine through a natural language. One of the earliest systems was ELIZA.
\end{df}
\begin{df}{Question answering}\sb{} is an \app{} of \nlp{} where a user asks a question who is resolved by the machine. An example of such systems is \emph{Watson (IBM)}.
\end{df}
\begin{df}{Automatic summarization}\sb{} is an \app{} of \nlp{} where a machine summarizes a fragment into a smaller fragment with approximately the same meaning. \sb{} comes in different flavors: \flv{single text summarization}, \flv{multi-document summarization} and \flv{multi lingual multi-document summarization}.
\end{df}
\begin{df}{Automatic paraphrasing}\sb{} is an \app{} of \nlp{}.
\end{df}
\begin{df}{Topic detection}\sb{} is an \app{} of \nlp{}.
\end{df}
\begin{df}{Authorship attribution}\sb{} is an \app{} of \nlp{} where a machine tries to detect plagiarism.
\end{df}
\begin{df}{Sentiment analysis}\sb{} is an \app{} of \nlp{} where a machine tries to guess what people think and houw they feel about certain concepts or products.
\end{df}
\begin{df}{Automatic paraphrasing}\sb{} is an \app{} of \nlp{}.
\end{df}
\begin{df}{Protocol}a \sb{} is a set of rules describing the behavior of the system under several circumstances.
\end{df}
\begin{df}{Machine learning technique}A \sb{} is an \tcq{} where a machine tries to learn a certain task empirically. \sb{}s come in two flavors: \flv{unsupervised techniques} and \flv{supervised techniques}.
\end{df}
\begin{df}{Unsupervised techniques}\sb{} are a set of machine learning techniques where a machine learns from unlabeled data. Since the output is not specified, this method requires lots of data. A huge advantage is that the given data can be generated easily. \sb{} however are only effective for a limited set of problems.
\end{df}
\begin{df}{Supervised techniques}\sb{} are a set of machine learning techniques where a machine learns from labeled data. Most machine learning techniques succeed in learning by using these techniques. Generating an annotated dataset however, can be quite cumbersome.
\end{df}
\begin{df}{Rule-based techniques}\sb{} are a set of techniques where a programmer specifies how the data will be processed based on a set of rules. This approach requires no data at all. Rule-based techniques can be quite cumbersome since most problems require a vast amount of rules. There rules are not always easy to derive and require a lot lot work.
\end{df}
\begin{df}{Hybrid approach}A \sb{} is a technique to tackle problems where both machine learning and rule-based techniques are combined. For instance, one can use machine learning techniques to learn human readable rules.
\end{df}
\begin{df}{Annotated data}\sb{} is a form of data where both the input and the expected output are combined as tuples in a single dataset. There exist different flavors of annotated data: \flv{Monolingual annotated data}, \flv{Multilingual annotated data}, \flv{Parallel annotated data} and \flv{Comparable annotated data}. Furthermore annotated data can introduce a divers set of annotation layers: \flv{Phonetic transcription annotation layer}, \flv{Prosodic annotation layer}, \flv{Morphological annotation layer}, \flv{Part-of-speech annotation layer}, \flv{Syntactic parsing annotation layer}, \flv{Semantic annotation layer} and \flv{Alignment annotation layer}.
\end{df}
\begin{df}{Monolingual annotated data}\sb{} is a form of annotated data where all data is in one and the same language.
\end{df}
\begin{df}{Multilingual annotated data}\sb{} is a form of annotated data where the data can be in any of a determined set of languages.
\end{df}
\begin{df}{Parallel annotated data}\sb{} is a form of annotated data that is at least bilingual. Parts of the document in one language can be considered translations of parts of documents in another language.
\end{df}
\begin{df}{Comparable annotated data}\sb{} is a form of annotated data where al data is about the same event (monolingual or multilingual).
\end{df}
\begin{df}{Phonetic transcription annotation layer}\sb{} is an annotation layer describing the text in phonetic characters.
\end{df}
\begin{df}{Prosodic annotation layer}\sb{} is an annotation layer describing the stress patterns in the fragment.
\end{df}
\begin{df}{Morphological annotation layer}\sb{} is an annotation layer describing the lemmatization, stemming and morphological aspects of the fragments.
\end{df}
\begin{df}{Part-of-speech annotation layer}\sb{} is an annotation layer describing the role of the different words in the fragments.
\end{df}
\begin{df}{Syntactic annotation layer}\sb{} is an annotation layer describing how the text should syntactically be parsed.
\end{df}
\begin{df}{Semantic annotation layer}\sb{} is an annotation layer describing the semantical meaning of each data element.
\end{df}
\begin{df}{Alignment annotation layer}\sb{} is an annotation layer describing where different parts of a certain fragment begin. For instance: \emph{paragraph}, \emph{sentence}, \emph{word}, \emph{subtree},...
\end{df}
\begin{df}{Bootstrapping}\sb{} is a method to speedup the development of an annotated database. In a first stage data is manually annotated. Based on this set of data one can use a rule-based system and annotated future fragments automatically and manually correct errors.
\end{df}
\begin{df}[Development set]{Training set}A \sb{} is a set of annotated data used to train a system with machine learning techniques. The \sb{} should never be used to test the system. Otherwise the system is tested with seen data.
\end{df}
\begin{df}[Evaluation set]{Test set}A \sb{} is a set of annotated data used to test a system. The \sb{} should never be used to train the system.
\end{df}
\begin{df}[Ground truth]{Gold Standard}A \sb{} is a set of perfect solutions (or oracle answers). One can test a system by comparing the output with the \sb{}. This set is never used to develop a system. Only the final evaluation is conducted with this dataset.
\end{df}
\begin{df}{Cross-validation}\sb{} is a method in order to test and train a system with the same set of (limited data). Each time $n-1$ parts out of $n$ of the dataset are used to train the system and $1$ part is used to test the system. This procedure is repeated $n$ times and the final score is the average over the $n$ runs.
\end{df}
\begin{df}{In vitro}\sb{} is a term used to describe we test the system in lab situations. In such situations we are interested in the accuracy, F-score, BLEU,...
\end{df}
\begin{df}{In vivo}\sb{} is a term used to describe we test the system in a real life setting. In such setting we are interested in the return of investment, process speed-up, quality improvement,...
\end{df}
\begin{df}{Accuracy}The \sb{} is a metric to measure the performance of a system. The \sb{} is the percentage of agreement between the output and the gold standard. The \sb{} is sensitive for the granularity of the evaluation together with \flv{human ceiling}. In \pos{} tagging, the \sb{} is the number of correctly tagged words divided by the total number of words.
\end{df}
\begin{df}{Token accuracy}\sb{} is a way to measure the performance of a system by measuring the accuracy of each lemmatized tokens. Therefore the word frequency is taken into account.
\end{df}
\begin{df}{Type accuracy}\sb{} is a way to measure the performance of a system by measuring the accuracy of each word. Therefore the word frequency is not taken into account.
\end{df}
\begin{df}{Precision}The \sb{} is an evaluation metric that measures the ratio between the true positives and the number of items who were classified as positive. Therefore it is the proportion of the selected items the system got right. In chunking the precision is the number of correct chunks given divided by the total number of chunks given by the system.
\end{df}
\begin{df}{Recall}The \sb{} is an evaluation metric that measures the ratio between the true positives and the number of positive items. Therefore it is the proportion of target items the system selected. In chunking, the recall is defined as the number of correct chunks given by the system divided by the total number of actual chunks in the text.
\end{df}
\begin{df}{F-score}The \sb{} is an evaluation metric that combines the \flv{precision} and \flv{recall}. Therefore the harmonic mean between these metrics is used.
\end{df}
\begin{df}{Error rate}The \sb{} is an evaluation metric equal to $1-\mbox{accuracy}$.
\end{df}
\begin{df}{Baseline}A \sb{} is the most frequent class in \pos{}-tagging.
\end{df}
\begin{df}{Correct chunk}A chunk in a fragment is correct if the chunk label is correct and both boundaries are correct as well.
\end{df}